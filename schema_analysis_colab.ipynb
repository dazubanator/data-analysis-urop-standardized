{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Schema Analysis Tool - Google Colab Notebook\n",
        "\n",
        "This notebook provides a complete, reproducible workflow for analyzing schema experiment data.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The analysis pipeline consists of:\n",
        "1. **Load & Merge**: Combine all CSV files from raw data\n",
        "2. **Clean**: Remove rows with missing group_id\n",
        "3. **Preprocess**: Rename faces, transform angles\n",
        "4. **Filter**: Apply angle rules (3-43°) and validate subjects\n",
        "5. **Balance**: Remove unmatched trials\n",
        "6. **Analyze**: Calculate D-values and statistics\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "First, we'll clone the repository and install the schema_analysis package."
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/BackyardBrains/schema-analysis.git\n",
        "\n",
        "# Install the package\n",
        "!pip install -q ./schema-analysis\n",
        "\n",
        "print(\"✓ Installation complete!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Required Libraries\n",
        "\n",
        "Import all necessary libraries for the analysis."
      ],
      "metadata": {
        "id": "import_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from schema_analysis.data_loader import load_and_merge_csvs\n",
        "from schema_analysis import TubeTrials\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Upload Your Data\n",
        "\n",
        "You have two options for providing your data:\n",
        "\n",
        "### Option A: Upload CSV files directly\n",
        "Run the cell below to upload your CSV files."
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data/raw', exist_ok=True)\n",
        "\n",
        "# Upload files\n",
        "print(\"Please upload your CSV files:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to data/raw directory\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f'data/raw/{filename}')\n",
        "    print(f\"✓ Moved {filename} to data/raw/\")\n",
        "\n",
        "print(\"\\n✓ Upload complete!\")"
      ],
      "metadata": {
        "id": "upload_files"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Option B: Mount Google Drive\n",
        "If your data is in Google Drive, run this cell instead and update the path."
      ],
      "metadata": {
        "id": "drive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update this path to point to your data folder in Google Drive\n",
        "DATA_DIR = '/content/drive/MyDrive/your-data-folder'\n",
        "\n",
        "print(f\"✓ Google Drive mounted! Data directory: {DATA_DIR}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Configuration\n",
        "\n",
        "Set the analysis parameters. These are the standard values used in the official analysis."
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis parameters\n",
        "DATA_DIR = 'data/raw'  # Change this if using Google Drive (Option B above)\n",
        "MIN_ANGLE = 3\n",
        "MAX_ANGLE = 43\n",
        "MAX_INVALID_TRIALS = 2\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Data directory: {DATA_DIR}\")\n",
        "print(f\"  Valid angle range: {MIN_ANGLE}° - {MAX_ANGLE}°\")\n",
        "print(f\"  Max invalid trials per subject: {MAX_INVALID_TRIALS}\")"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run the Analysis\n",
        "\n",
        "This cell runs the complete standardized analysis pipeline."
      ],
      "metadata": {
        "id": "analysis_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"STANDARDIZED ANALYSIS PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Load & Merge Data\n",
        "print(\"\\n[STEP 1] Loading and merging CSV files...\")\n",
        "merged_df = load_and_merge_csvs(DATA_DIR)\n",
        "print(f\"Loaded {len(merged_df)} total trials\")\n",
        "\n",
        "# Step 2: Clean Data\n",
        "print(\"\\n[STEP 2] Cleaning data (removing missing group_id)...\")\n",
        "initial_count = len(merged_df)\n",
        "merged_df = merged_df.dropna(subset=['session_group'])\n",
        "cleaned_count = len(merged_df)\n",
        "dropped = initial_count - cleaned_count\n",
        "print(f\"Removed {dropped} rows with missing session_group\")\n",
        "print(f\"Remaining: {cleaned_count} trials\")\n",
        "\n",
        "# Step 3-6: Process with TubeTrials\n",
        "trials = TubeTrials(merged_df)\n",
        "\n",
        "print(f\"\\n[STEP 3] Preprocessing (rename faces, transform angles)...\")\n",
        "trials.process_angles()\n",
        "print(f\"✓ Angles processed\")\n",
        "\n",
        "print(f\"\\n[STEP 4] Filtering trials...\")\n",
        "print(f\"  Applying angle rule: {MIN_ANGLE} < end_angle < {MAX_ANGLE}\")\n",
        "trials.mark_valid_angles(min_angle=MIN_ANGLE, max_angle=MAX_ANGLE)\n",
        "\n",
        "print(f\"  Validating subjects (max {MAX_INVALID_TRIALS} invalid trials)...\")\n",
        "trials.mark_valid_subjects(max_invalid_trials=MAX_INVALID_TRIALS)\n",
        "\n",
        "print(f\"\\n[STEP 5] Selecting valid trials...\")\n",
        "clean_trials = trials.select(valid_only=True)\n",
        "print(f\"Clean trials: {len(clean_trials)}\")\n",
        "\n",
        "print(f\"\\n[STEP 6] Balancing (removing unmatched trials)...\")\n",
        "results = clean_trials.calc_d_values()\n",
        "\n",
        "# Step 7: Validity Report & Statistics\n",
        "print(f\"\\n[STEP 7] Attrition Report & Statistics...\")\n",
        "trials.get_validity_stats()\n",
        "stats_df = clean_trials.calc_stats()\n",
        "\n",
        "print(f\"Valid pairs: {len(results)}\")\n",
        "print(f\"Paired trials: {len(results) * 2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "run_analysis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. View Results\n",
        "\n",
        "Display the calculated D-values and statistics."
      ],
      "metadata": {
        "id": "results_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"D-Value Pairs (first 20):\")\n",
        "print(\"=\" * 70)\n",
        "display(results.head(20))\n",
        "\n",
        "print(\"\\n\\nStatistics by Face ID:\")\n",
        "print(\"=\" * 70)\n",
        "display(stats_df)"
      ],
      "metadata": {
        "id": "view_results"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Visualize Results\n",
        "\n",
        "Create visualizations to better understand the data."
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Distribution of D-values\n",
        "axes[0, 0].hist(results['d'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].axvline(results['d'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {results[\"d\"].mean():.2f}')\n",
        "axes[0, 0].set_xlabel('D-value (degrees)', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Frequency (D-values)', fontsize=12)\n",
        "axes[0, 0].set_title('Distribution of D-values (All Faces)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. D-values by Face ID\n",
        "face_order = sorted(results['face_id'].unique())\n",
        "sns.boxplot(data=results, x='face_id', y='d', order=face_order, ax=axes[0, 1])\n",
        "axes[0, 1].set_xlabel('Face ID', fontsize=12)\n",
        "axes[0, 1].set_ylabel('D-value (degrees)', fontsize=12)\n",
        "axes[0, 1].set_title('D-values by Face ID', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Mean D-value by Face ID with error bars\n",
        "x_pos = range(len(stats_df))\n",
        "axes[1, 0].bar(x_pos, stats_df['mean'], yerr=stats_df['sem'], \n",
        "               capsize=5, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_xticks(x_pos)\n",
        "axes[1, 0].set_xticklabels(stats_df['face_id'])\n",
        "axes[1, 0].set_xlabel('Face ID', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Mean D-value (degrees)', fontsize=12)\n",
        "axes[1, 0].set_title('Mean D-value by Face ID (±SEM)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. P-values by Face ID\n",
        "colors = ['green' if p < 0.05 else 'red' for p in stats_df['p_value']]\n",
        "axes[1, 1].bar(x_pos, stats_df['p_value'], color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].axhline(0.05, color='black', linestyle='--', linewidth=2, label='p = 0.05')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(stats_df['face_id'])\n",
        "axes[1, 1].set_xlabel('Face ID', fontsize=12)\n",
        "axes[1, 1].set_ylabel('P-value', fontsize=12)\n",
        "axes[1, 1].set_title('Statistical Significance by Face ID', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Visualizations generated!\")"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Download Results\n",
        "\n",
        "Save the results to CSV files and download them."
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create results directory\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# Save results\n",
        "results_file = 'results/d_values.csv'\n",
        "stats_file = 'results/statistics.csv'\n",
        "\n",
        "results.to_csv(results_file, index=False)\n",
        "stats_df.to_csv(stats_file, index=False)\n",
        "\n",
        "print(\"Results saved to:\")\n",
        "print(f\"  - {results_file}\")\n",
        "print(f\"  - {stats_file}\")\n",
        "\n",
        "# Download files\n",
        "print(\"\\nDownloading files...\")\n",
        "files.download(results_file)\n",
        "files.download(stats_file)\n",
        "\n",
        "print(\"\\n✓ Download complete!\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Summary Statistics\n",
        "\n",
        "Display a comprehensive summary of the analysis."
      ],
      "metadata": {
        "id": "summary_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nData Processing:\")\n",
        "print(f\"  Initial trials: {initial_count}\")\n",
        "print(f\"  After cleaning: {cleaned_count}\")\n",
        "print(f\"  After filtering: {len(clean_trials)}\")\n",
        "print(f\"  Valid pairs: {len(results)}\")\n",
        "print(f\"  Paired trials: {len(results) * 2}\")\n",
        "\n",
        "print(\"\\nOverall Statistics:\")\n",
        "print(f\"  Mean D-value: {results['d'].mean():.4f}°\")\n",
        "print(f\"  Std Dev: {results['d'].std():.4f}°\")\n",
        "print(f\"  SEM: {results['d'].sem():.4f}°\")\n",
        "print(f\"  Min D-value: {results['d'].min():.4f}°\")\n",
        "print(f\"  Max D-value: {results['d'].max():.4f}°\")\n",
        "\n",
        "print(\"\\nFace IDs Analyzed:\")\n",
        "for _, row in stats_df.iterrows():\n",
        "    sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
        "    print(f\"  {row['face_id']}: Mean={row['mean']:.4f}°, SEM={row['sem']:.4f}, p={row['p_value']:.4f} {sig}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Legend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "summary"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Optional: Verification with Dummy Data\n",
        "\n",
        "The following cells allow you to verify the analysis pipeline using dummy data with known expected results."
      ],
      "metadata": {
        "id": "verification_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download verification script from repository\n",
        "!wget -q https://raw.githubusercontent.com/BackyardBrains/schema-analysis/main/verification/run_verification.py\n",
        "\n",
        "# Run verification\n",
        "!python run_verification.py\n",
        "\n",
        "print(\"\\n✓ Verification complete! If all tests passed, the analysis pipeline is working correctly.\")"
      ],
      "metadata": {
        "id": "verification"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Need Help?\n",
        "\n",
        "- **Repository**: [BackyardBrains/schema-analysis](https://github.com/BackyardBrains/schema-analysis)\n",
        "- **Documentation**: See the `README.md` and `verification/README.md` in the repository\n",
        "- **Issues**: Report bugs or ask questions on the GitHub Issues page\n",
        "\n",
        "---\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this analysis tool in your research, please cite:\n",
        "\n",
        "```\n",
        "Schema Analysis Tool\n",
        "https://github.com/BackyardBrains/schema-analysis\n",
        "```"
      ],
      "metadata": {
        "id": "help_header"
      }
    }
  ]
}